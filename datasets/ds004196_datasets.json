{
  "dataset_id": "ds004196",
  "date_retrieved": "2025-07-31T00:20:04.862538+00:00",
  "dataset_description": {
    "Name": "Bimodal dataset on Inner speech",
    "BIDSVersion": "1.4",
    "License": "CC0",
    "Authors": [
      "Foteini Liwicki",
      "Vibha Gupta",
      "Rajkumar Saini",
      "Kanjar De",
      "Nosheen Abid",
      "Sumit Rakesh",
      "Scott Wellington",
      "Holly Wilson",
      "Marcus Liwicki",
      "Johan Eriksson"
    ],
    "HowToAcknowledge": "Foteini Simistira Liwicki, Vibha Gupta, Rajkumar Saini, Kanjar De, Nosheen Abid, Sumit Rakesh, Scott Wellington, Holly Wilson, Marcus Liwicki, Johan Eriksson (2022). Bimodal dataset on inner speech. OpenNeuro. [Dataset] doi:10.18112/openneuro.ds004196.v1.0.29",
    "ReferencesAndLinks": [
      "https://github.com/LTU-Machine-Learning/Inner_Speech_EEG_FMRI",
      "https://www.biorxiv.org/content/10.1101/2022.05.24.492109v3"
    ],
    "DatasetDOI": "doi:10.18112/openneuro.ds004196.v2.0.2",
    "DatasetType": "raw",
    "EthicsApprovals": [
      "Etikpr√∂vning myndigheden, ID:2021-06710-01"
    ],
    "Acknowledgements": "We would like to thank the Stockholm University Brain Imaging Centre (SUBIC) and more precisely Rita Almeida,  Patrik Andersson for giving us access to their facilities and for supporting us in this endeavor.\nPetter Kallioinen and Christoffer Schiehe-Forbes for their valuable support during the EEG data acquisition.\nFinally, we would also like to thank all participants for taking part in this study. ",
    "Funding": [
      "This research was funded by the Grants for excellent research projects proposals of SRT.ai 2022."
    ]
  },
  "readme_content": "Bimodal dataset on Inner Speech\n\nCode available: https://github.com/LTU-Machine-Learning/Inner_Speech_EEG_FMRI \n\nPublication available: https://www.nature.com/articles/s41597-023-02286-w\n\nAbstract:\nThe recognition of inner speech, which could give a `voice' to patients that have no ability to speak or move, is a challenge for brain-computer interfaces (BCIs). A shortcoming of the available datasets is that they do not combine modalities to increase the performance of inner speech recognition. Multimodal datasets of brain data enable the fusion of neuroimaging modalities with complimentary properties, such as the high spatial resolution of functional magnetic resonance imaging (fMRI) and the temporal resolution of electroencephalography (EEG), and therefore are promising for decoding inner speech. This paper presents the first publicly available bimodal dataset containing EEG and fMRI data acquired nonsimultaneously during inner-speech production.  Data were obtained from four healthy, right-handed participants during an inner-speech task with words in either a social or numerical category. Each of the 8-word stimuli were assessed with 40 trials, resulting in 320 trials in each modality for each participant.   \nThe aim of this work is to provide a publicly available bimodal dataset on inner speech, contributing towards speech prostheses.\n\nShort Dataset description:\nThe dataset consists of 1280 trials in each modality (EEG, FMRI). \nThe stimuli contain 8 words,  selected from 2 different categories (social, numeric):\nSocial: child, daughter, father, wife\nNumeric: four, three, ten, six\n\nThere are 4 subjects in total: sub-01, sub-02, sub-03, sub-05. Initially, there were 5 participants, however, sub-04 data was rejected due to high fluctuations. Details of valid data are available in the file participants.tsv. \n\nFor questions please contact: foteini.liwicki@ltu.se ",
  "github_info": {
    "repository_url": "https://github.com/OpenNeuroDatasets/ds004196",
    "exists": true,
    "description": "OpenNeuro dataset - Bimodal dataset on Inner speech",
    "created_at": "2023-04-26T11:21:54+00:00",
    "updated_at": "2023-05-23T08:00:14+00:00",
    "default_branch": "main"
  },
  "retrieval_status": {
    "dataset_description": "not_found",
    "readme": "not_found",
    "repository": "success"
  }
}